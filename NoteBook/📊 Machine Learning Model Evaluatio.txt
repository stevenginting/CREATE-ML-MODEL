ğŸ“Š Machine Learning Model Evaluation

Evaluating a machine learning model is just as important as building one.
Different problems â†’ different models â†’ different evaluation metrics.

ğŸ”¹ Classification Model Evaluation Metrics

Accuracy â†’ Proportion of correct predictions.

Best when classes are balanced.

Range: 0 â€“ 1 (1 = perfect).

Precision â†’ Of all predicted positives, how many are actually correct.

High when false positives are low.

Recall â†’ Of all actual positives, how many were correctly predicted.

High when false negatives are low.

F1-score â†’ Harmonic mean of precision and recall.

Useful for imbalanced datasets.

Confusion Matrix â†’ Table comparing predictions vs actual values.

Perfect predictions = diagonal line.

Cross-validation â†’ Splits data into folds, trains/tests on each, averages performance.

Classification Report (sklearn) â†’ Outputs precision, recall, F1-score.

ROC Curve â†’ Plot of True Positive Rate vs False Positive Rate.

AUC Score â†’ Area under the ROC curve.

1.0 = perfect model.

ğŸ“Œ Choosing classification metrics:

Balanced classes â†’ use accuracy.

Imbalanced classes â†’ focus on precision, recall, F1-score.

If false positives are worse â†’ aim for higher precision.

If false negatives are worse â†’ aim for higher recall.

Always use a confusion matrix for visualization.

ğŸ”¹ Regression Model Evaluation Metrics

RÂ² (Coefficient of Determination)

Compares predictions to the mean of target values.

Range: -âˆ to 1.

0 = model only predicts the mean.

1 = perfect predictions.

Mean Absolute Error (MAE)

Average of absolute differences between predictions and actual values.

Interpretable as the â€œaverage mistake size.â€

Mean Squared Error (MSE)

Average of squared differences.

Punishes larger errors more heavily (outliers matter more).

ğŸ“Œ Choosing regression metrics:

RÂ² â†’ quick check, like accuracy.

MAE â†’ when being off by 10,000 is exactly twice as bad as being off by 5,000.

MSE â†’ when being off by 10,000 is more than twice as bad as being off by 5,000.